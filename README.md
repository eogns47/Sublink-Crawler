# ğŸ”Dynamic link crawler

ğŸš€You Can find All Urls from base url!ğŸš€ <br>
Dynamic web crawler that uses dynamic browser (Puppeteer) which fetches all links on a page and its children. <br>

## ğŸ’¡How to use:

1. Clone the repo and run `npm install puppeteer yargs`
2. Create a file that lists scrapped targets on {root}/inputs/targets.txt
3. Create a file that lists unscrapped targets on {root}/inputs/blacklist.txt
4. Run `node index.js -t targets.txt -r results.txt -b blacklist.txt -d 1`

5. Install Unit test tool `npm install --save-dev jest`
6. Create a file that lists scrapped targets on {root}/inputs/targets.txt
7. Create a file that lists unscrapped targets on {root}/inputs/blacklist.txt
8. Run `node index.js -t targets.txt -r results.txt -b blacklist.txt -d 1`

Options: <br>
`--version` Show version number [boolean] <br>
`-t` Input file path [required] <br>
`-u` Targets array list <br>
`-r` Output file path <br>
`-d` Crawling depth <br>
`-b` Blacklist file path to prevent an url for being crawled (hard match)<br>
`--full` Use full url for crawling instead of its base<br>
`-v` Verbosity level [boolean]<br>
`--base` Must include the base url to except external links when crawling [boolean]<br>
<br>
`-h, --help  Show help` [boolean]<br>

## ğŸ› ï¸Tech Stack:

![link crawler Architecture drawio (1)](https://github.com/eogns47/Sublink-Crawler/assets/102205852/3df0b92d-749b-4935-adb9-fa01326d1615)

## ğŸŒ²File Structure

```
link-crawler
â”œâ”€ Logger
â”‚  â””â”€ logger.js
â”œâ”€ README.md
â”œâ”€ babel.config.js
â”œâ”€ inputs
â”‚  â””â”€ .gitkeep
â”œâ”€ jest.config.js
â”œâ”€ logs
â”‚  â””â”€ .gitkeep
â”œâ”€ node_modules
â”œâ”€ package-lock.json
â”œâ”€ package.json
â”œâ”€ results
â”‚  â””â”€ .gitkeep
â”œâ”€ src
â”‚  â”œâ”€ Config
â”‚  â”‚  â””â”€ Extensions.js
â”‚  â”œâ”€ IOView.js
â”‚  â”œâ”€ LinkCrawler.js
â”‚  â”œâ”€ LinkPreprocessor.js
â”‚  â”œâ”€ index.js
â”‚  â””â”€ messageHandler.js
â””â”€ yarn.lock

```
